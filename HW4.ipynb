{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTIZFaAtanKwQyNkS4+3QC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidanbolinger/ArtificialIntelligence/blob/main/HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIqVjwGZ-3cI",
        "outputId": "c716a983-9db3-4b4c-f0ed-7f40620ad0ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset:\n",
            "   Free  Win  Hello  Click  Offer     Label\n",
            "0     1    1      0      1      1      Spam\n",
            "1     0    1      0      1      1      Spam\n",
            "2     1    0      1      0      0  Not Spam\n",
            "3     0    0      1      0      0  Not Spam\n",
            "4     1    1      0      1      0      Spam\n",
            "5     0    0      1      0      0  Not Spam\n",
            "\n",
            "Prior Probabilities:\n",
            "{'Spam': 0.5, 'Not Spam': 0.5}\n",
            "\n",
            "Conditional Probabilities (with Laplace Smoothing):\n",
            "Spam: {'Free': np.float64(0.6), 'Win': np.float64(0.8), 'Hello': np.float64(0.2), 'Click': np.float64(0.8), 'Offer': np.float64(0.6)}\n",
            "Not Spam: {'Free': np.float64(0.4), 'Win': np.float64(0.2), 'Hello': np.float64(0.8), 'Click': np.float64(0.2), 'Offer': np.float64(0.2)}\n",
            "\n",
            "Naive Bayes Classifier Function calculation:\n",
            "Prediction for new email: Spam\n"
          ]
        }
      ],
      "source": [
        "# Question 4\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Print and verify the dataset\n",
        "data = pd.DataFrame({\n",
        "    'Free':    [1, 0, 1, 0, 1, 0],\n",
        "    'Win':     [1, 1, 0, 0, 1, 0],\n",
        "    'Hello':   [0, 0, 1, 1, 0, 1],\n",
        "    'Click':   [1, 1, 0, 0, 1, 0],\n",
        "    'Offer':   [1, 1, 0, 0, 0, 0],\n",
        "    'Label':   ['Spam', 'Spam', 'Not Spam', 'Not Spam', 'Spam', 'Not Spam']\n",
        "})\n",
        "\n",
        "print(\"Dataset:\")\n",
        "print(data)\n",
        "\n",
        "# 2. Compute prior probabilities\n",
        "def compute_priors(df):\n",
        "    total = len(df)\n",
        "    priors = df['Label'].value_counts().to_dict()\n",
        "    return {label: count / total for label, count in priors.items()}\n",
        "\n",
        "priors = compute_priors(data)\n",
        "print(\"\\nPrior Probabilities:\")\n",
        "print(priors)\n",
        "\n",
        "# 3. Compute conditional probabilities with Laplace smoothing\n",
        "def compute_conditional_probs(df, features, label_col='Label'):\n",
        "    cond_probs = {}\n",
        "    labels = df[label_col].unique()\n",
        "    for label in labels:\n",
        "        subset = df[df[label_col] == label]\n",
        "        label_probs = {}\n",
        "        for feature in features:\n",
        "            # Laplace smoothing: add 1 to numerator, +2 to denominator (binary features)\n",
        "            prob_feature_given_label = (subset[feature].sum() + 1) / (len(subset) + 2)\n",
        "            label_probs[feature] = prob_feature_given_label\n",
        "        cond_probs[label] = label_probs\n",
        "    return cond_probs\n",
        "\n",
        "features = ['Free', 'Win', 'Hello', 'Click', 'Offer']\n",
        "cond_probs = compute_conditional_probs(data, features)\n",
        "\n",
        "print(\"\\nConditional Probabilities (with Laplace Smoothing):\")\n",
        "for label in cond_probs:\n",
        "    print(f\"{label}: {cond_probs[label]}\")\n",
        "\n",
        "# 4. Naïve Bayes Classifier Function\n",
        "def classify(email, priors, cond_probs):\n",
        "    log_probs = {}\n",
        "    for label in priors:\n",
        "        log_prob = np.log(priors[label])  # Start with log prior\n",
        "        for feature in email:\n",
        "            prob = cond_probs[label][feature]\n",
        "            # If feature is 1, use P(feature=1 | label); else use P(feature=0 | label) = 1 - P(feature=1 | label)\n",
        "            log_prob += np.log(prob if email[feature] == 1 else (1 - prob))\n",
        "        log_probs[label] = log_prob\n",
        "    return max(log_probs, key=log_probs.get)\n",
        "\n",
        "# Test email\n",
        "new_email = {\n",
        "    'Free': 1,\n",
        "    'Win': 1,\n",
        "    'Hello': 0,\n",
        "    'Click': 1,\n",
        "    'Offer': 0\n",
        "}\n",
        "\n",
        "prediction = classify(new_email, priors, cond_probs)\n",
        "print(\"\\nNaive Bayes Classifier Function calculation:\")\n",
        "print(f\"Prediction for new email: {prediction}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 5\n",
        "import numpy as np\n",
        "\n",
        "# Training frequency counts for each feature per class\n",
        "training_data = {\n",
        "    'Outlook': {\n",
        "        'Sunny': {'Yes': 2, 'No': 3},\n",
        "        'Overcast': {'Yes': 4, 'No': 0},\n",
        "        'Rain': {'Yes': 3, 'No': 2}\n",
        "    },\n",
        "    'Temperature': {\n",
        "        'Hot': {'Yes': 2, 'No': 2},\n",
        "        'Mild': {'Yes': 4, 'No': 2},\n",
        "        'Cool': {'Yes': 3, 'No': 1}\n",
        "    },\n",
        "    'Humidity': {\n",
        "        'High': {'Yes': 3, 'No': 4},\n",
        "        'Normal': {'Yes': 6, 'No': 1}\n",
        "        # 'Low' not seen in training\n",
        "    },\n",
        "    'Wind': {\n",
        "        'Weak': {'Yes': 6, 'No': 2},\n",
        "        'Strong': {'Yes': 3, 'No': 3}\n",
        "        # 'Gusty' not seen in training\n",
        "    }\n",
        "}\n",
        "\n",
        "# Total class counts\n",
        "class_totals = {\n",
        "    'Yes': 9,\n",
        "    'No': 5\n",
        "}\n",
        "\n",
        "# All possible values per feature (used for smoothing)\n",
        "feature_values = {\n",
        "    'Outlook': ['Sunny', 'Overcast', 'Rain'],\n",
        "    'Temperature': ['Hot', 'Mild', 'Cool'],\n",
        "    'Humidity': ['High', 'Normal', 'Low'],\n",
        "    'Wind': ['Weak', 'Strong', 'Gusty']\n",
        "}\n",
        "\n",
        "# Test instance\n",
        "X_test = {\n",
        "    'Outlook': 'Sunny',\n",
        "    'Temperature': 'Mild',\n",
        "    'Humidity': 'Low',\n",
        "    'Wind': 'Gusty'\n",
        "}\n",
        "\n",
        "def build_conditional_prob_tables(data, totals, features):\n",
        "    prob_tables = {}\n",
        "    for feature in features:\n",
        "        prob_tables[feature] = {}\n",
        "        for value in features[feature]:\n",
        "            prob_tables[feature][value] = {}\n",
        "            for label in ['Yes', 'No']:\n",
        "                count = data.get(feature, {}).get(value, {}).get(label, 0)\n",
        "                k = len(features[feature])\n",
        "                smoothed_prob = (count + 1) / (totals[label] + k)\n",
        "                prob_tables[feature][value][label] = smoothed_prob\n",
        "    return prob_tables\n",
        "\n",
        "def compute_posteriors(X_test, cond_probs, class_totals):\n",
        "    total = sum(class_totals.values())\n",
        "    posteriors = {}\n",
        "    for label in ['Yes', 'No']:\n",
        "        log_prob = np.log(class_totals[label] / total)\n",
        "        for feature, value in X_test.items():\n",
        "            prob = cond_probs[feature][value][label]\n",
        "            log_prob += np.log(prob)\n",
        "        posteriors[label] = log_prob\n",
        "    return posteriors\n",
        "\n",
        "# Step 1: Build conditional probability tables with Laplace smoothing\n",
        "conditional_probs = build_conditional_prob_tables(training_data, class_totals, feature_values)\n",
        "\n",
        "# Step 2 & 3: Compute posteriors and make prediction\n",
        "posterior_probs = compute_posteriors(X_test, conditional_probs, class_totals)\n",
        "\n",
        "# Output\n",
        "print(\"Posterior Log Probabilities:\")\n",
        "for label, log_prob in posterior_probs.items():\n",
        "    print(f\"{label}: {log_prob:.4f}\")\n",
        "\n",
        "prediction = max(posterior_probs, key=posterior_probs.get)\n",
        "print(f\"\\nPrediction: Play Tennis? ➤ {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTmiCJ-qIY3j",
        "outputId": "b0c02ac4-9f67-4d60-e930-f4e513547cf3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Posterior Log Probabilities:\n",
            "Yes: -7.6734\n",
            "No: -6.8625\n",
            "\n",
            "Prediction: Play Tennis? ➤ No\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Initialize\n",
        "weights = np.array([0.0, 0.0])\n",
        "bias = 0.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Step 2: Define data\n",
        "X = np.array([\n",
        "    [1.4, 0.2], [1.5, 0.3], [1.3, 0.2], [1.6, 0.4], [1.4, 0.3],  # Setosa (-1)\n",
        "    [4.5, 1.5], [4.7, 1.6], [4.6, 1.4], [4.9, 1.5], [5.1, 1.8]   # Versicolor (+1)\n",
        "])\n",
        "y = np.array([-1, -1, -1, -1, -1, +1, +1, +1, +1, +1])\n",
        "\n",
        "#Training loop\n",
        "epochs = 20\n",
        "updates = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for xi, yi in zip(X, y):\n",
        "        prediction = np.sign(np.dot(weights, xi) + bias)\n",
        "        if prediction == 0:\n",
        "            prediction = 1  # treat zero as positive\n",
        "        if prediction != yi:\n",
        "            weights += learning_rate * yi * xi\n",
        "            bias += learning_rate * yi\n",
        "            updates += 1\n",
        "\n",
        "# Step 3:Final weights and bias\n",
        "print(\"Final weights:\", weights)\n",
        "print(\"Final bias:\", bias)\n",
        "\n",
        "#Predictions\n",
        "predictions = np.sign(np.dot(X, weights) + bias)\n",
        "predictions[predictions == 0] = 1\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "#Number of updates\n",
        "print(\"Number of updates during training:\", updates)\n",
        "\n",
        "# Step 4: Confusion Matrix\n",
        "TP = sum((predictions == 1) & (y == 1))\n",
        "TN = sum((predictions == -1) & (y == -1))\n",
        "FP = sum((predictions == 1) & (y == -1))\n",
        "FN = sum((predictions == -1) & (y == 1))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(\"TP:\", TP)\n",
        "print(\"TN:\", TN)\n",
        "print(\"FP:\", FP)\n",
        "print(\"FN:\", FN)\n",
        "\n",
        "# Step 5: Metrics\n",
        "accuracy = (TP + TN) / len(y)\n",
        "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIuM3fTeP-Ud",
        "outputId": "1ef83011-9c01-49f8-f721-558a1243c776"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final weights: [0.02 0.08]\n",
            "Final bias: -0.2\n",
            "Predictions: [-1. -1. -1. -1. -1.  1.  1.  1.  1.  1.]\n",
            "Number of updates during training: 4\n",
            "\n",
            "Confusion Matrix:\n",
            "TP: 5\n",
            "TN: 5\n",
            "FP: 0\n",
            "FN: 0\n",
            "\n",
            "Evaluation Metrics:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n"
          ]
        }
      ]
    }
  ]
}